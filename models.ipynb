{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 22:35:23.454226: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM,SimpleRNN, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, MaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsDetector:\n",
    "    def __init__(self, embedding_path, embedding_dim=100, max_length=150):\n",
    "        self.embedding_path = embedding_path\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_length = max_length\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.word_index = None\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        text = str(text)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = word_tokenize(text)\n",
    "        words = [word.lower() for word in words if word.isalpha()]\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        preprocessed_text = ' '.join(words)\n",
    "\n",
    "        return preprocessed_text\n",
    "\n",
    "    def create_embedding_matrix(self, word_index, embedding_dict, embedding_dim):\n",
    "        embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "\n",
    "        for word, i in word_index.items():\n",
    "            if i > len(word_index):\n",
    "                continue\n",
    "            embedding_vector = embedding_dict.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        return embedding_matrix\n",
    "\n",
    "    def train_model(self, data, model_type='default'):\n",
    "        train_df, test_df = train_test_split(data, test_size=0.2, stratify=data['label'])\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.25, stratify=train_df['label'])\n",
    "\n",
    "        train_text = train_df['title'].apply(self.preprocess_text)\n",
    "        val_text = val_df['title'].apply(self.preprocess_text)\n",
    "        test_text = test_df['title'].apply(self.preprocess_text)\n",
    "\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.tokenizer.fit_on_texts(train_text)\n",
    "\n",
    "        train_sequences = self.tokenizer.texts_to_sequences(train_text)\n",
    "        val_sequences = self.tokenizer.texts_to_sequences(val_text)\n",
    "        test_sequences = self.tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "        X_train = pad_sequences(train_sequences, maxlen=self.max_length, padding='post', truncating='post')\n",
    "        X_val = pad_sequences(val_sequences, maxlen=self.max_length, padding='post', truncating='post')\n",
    "        X_test = pad_sequences(test_sequences, maxlen=self.max_length, padding='post', truncating='post')\n",
    "\n",
    "        y_train = train_df['label'].values\n",
    "        y_val = val_df['label'].values\n",
    "        y_test = test_df['label'].values\n",
    "\n",
    "        embedding_dict = {}\n",
    "        with open(self.embedding_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embedding_dict[word] = vector\n",
    "\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "        embedding_matrix = self.create_embedding_matrix(self.word_index, embedding_dict, self.embedding_dim)\n",
    "\n",
    "        if model_type == 'default':\n",
    "            self.model = Sequential([\n",
    "                Embedding(len(self.word_index) + 1, self.embedding_dim, weights=[embedding_matrix], trainable=False),\n",
    "                Conv1D(64, kernel_size=2, activation='relu'),\n",
    "                MaxPooling1D(pool_size=2),\n",
    "                Bidirectional(LSTM(128, return_sequences=False, kernel_regularizer=l2(0.01))),\n",
    "                Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "                Dropout(0.3),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        elif model_type == 'rnn':\n",
    "            self.model = Sequential([\n",
    "                Embedding(len(self.word_index) + 1, self.embedding_dim, weights=[embedding_matrix], trainable=False),\n",
    "                SimpleRNN(128, return_sequences=False, kernel_regularizer=l2(0.01)),\n",
    "                Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "                Dropout(0.3),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        elif model_type == 'lstm':\n",
    "            self.model = Sequential([\n",
    "                Embedding(len(self.word_index) + 1, self.embedding_dim, weights=[embedding_matrix], trainable=False),\n",
    "                Dropout(0.2),\n",
    "                LSTM(100, return_sequences=False, kernel_regularizer=l2(0.01)),\n",
    "                Dense(4, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        elif model_type == 'bilstm':\n",
    "            self.model = Sequential([\n",
    "                Embedding(len(self.word_index) + 1, self.embedding_dim, weights=[embedding_matrix], trainable=False),\n",
    "                Bidirectional(LSTM(128, return_sequences=False, kernel_regularizer=l2(0.01))),\n",
    "                Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "                Dropout(0.3),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        elif model_type == 'cnn':\n",
    "            self.model = Sequential([\n",
    "                Embedding(len(self.word_index) + 1, self.embedding_dim, weights=[embedding_matrix], trainable=False),\n",
    "                Conv1D(64, kernel_size=2, activation='relu'),\n",
    "                MaxPooling1D(pool_size=2),\n",
    "                Flatten(),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        elif model_type == 'cnn-rnn':\n",
    "            self.model = Sequential([\n",
    "                Embedding(len(self.word_index) + 1, self.embedding_dim, weights=[embedding_matrix], trainable=False),\n",
    "                Conv1D(64, kernel_size=2, activation='relu'),\n",
    "                MaxPooling1D(pool_size=2),\n",
    "                LSTM(128, return_sequences=False, kernel_regularizer=l2(0.01)),\n",
    "                Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "                Dropout(0.3),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=4, restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', patience=2, factor=0.1, verbose=1\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=15, \n",
    "            callbacks=[early_stopping, reduce_lr], \n",
    "            batch_size=64, \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        test_loss, test_accuracy = self.model.evaluate(X_test, y_test)\n",
    "        test_predictions = (self.model.predict(X_test) >= 0.5).astype(int)\n",
    "        test_precision = precision_score(y_test, test_predictions)\n",
    "        test_recall = recall_score(y_test, test_predictions)\n",
    "        test_f1 = f1_score(y_test, test_predictions)\n",
    "\n",
    "        print(\"Test Loss:\", test_loss)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "        print(\"Test Precision:\", test_precision)\n",
    "        print(\"Test Recall:\", test_recall)\n",
    "        print(\"Test F1-score:\", test_f1)\n",
    "\n",
    "\n",
    "    def predict(self, text):\n",
    "        preprocessed_text = self.preprocess_text(text)\n",
    "        sequence = self.tokenizer.texts_to_sequences([preprocessed_text])\n",
    "        padded_sequence = pad_sequences(sequence,maxlen=self.max_length, padding='post', truncating='post')\n",
    "        prediction = self.model.predict(padded_sequence)\n",
    "        return prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "data_path = 'ISOT'\n",
    "\n",
    "fake_df = pd.read_csv(data_path + '/Fake.csv')\n",
    "true_df = pd.read_csv(data_path + '/True.csv')\n",
    "\n",
    "fake_df = fake_df[['title']]\n",
    "true_df = true_df[['title']]\n",
    "\n",
    "true_df['label'] = 1\n",
    "fake_df['label'] = 0\n",
    "\n",
    "combined_df = pd.concat([true_df, fake_df], ignore_index=True)\n",
    "balanced_data = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "embedding_path =  'glove.6B.100d.txt'\n",
    "detector = FakeNewsDetector(embedding_path)\n",
    "detector.train_model(balanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    news_text = input(\"Enter a news title to check its authenticity (or 'quit' to exit): \")\n",
    "    if news_text.lower() == 'quit':\n",
    "        break\n",
    "    prediction = detector.predict(news_text)\n",
    "    if prediction >= 0.5:\n",
    "        print(\"True news.\")\n",
    "    else:\n",
    "        print(\"False news.\")\n",
    "    print(\"Prediction:\", prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
